{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network for ensemble tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble represents one of the most powerful intuition in data science. This technique combines multiple classifier predictions in order to obtain more accurate ones (at least in most of cases). But how can we combine prediction? We have two different technique for this purpose: hard voting and soft voting.\n",
    "\n",
    "Hard voting output as prediction the ensemble the *mode* value of the classifiers' predictions distribution, in other words the class that has received more votes. On the other hand soft voting compute the probability for an instance to belong to each class averaging the probabilty predicted by the classifiers. This lead to smoother results, giving more weight to classifier confident of what they are predicting. Probability are generally averaged with a simple equally weighted mean.\n",
    "\n",
    "The idea of this notebook is to use a deep neural network to obtain a more efficient weights' distribution for the soft voting method, in order to optimize the overall accuracy of the model. As base estimator i am going to use logistic regression as a classifier, since it is able to output the probability for each class through the sigmoid function. Each tree will be trained on a different sample of the original dataset, in order to reduce the likelyhood of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.rc_context at 0x14a4ed10978>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# libraries for uploading data \n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# setting random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Style setup\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize = 14)\n",
    "mpl.rc('xtick', labelsize = 16)\n",
    "mpl.rc('ytick', labelsize = 12)\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xkcd(False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the fashion mnist dataset from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(images_train, labels_train), (images_valid, labels_valid) = fashion_mnist.load_data()\n",
    "images_train = images_train.reshape((-1, 28 * 28)) / 255\n",
    "images_valid = images_valid.reshape((-1, 28 * 28)) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of the project i am going to implement from scratch an ensemble. The clfs variable is a dictionary which contains the trained estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n"
     ]
    }
   ],
   "source": [
    "def random_sample(X_set, y_set, length = 5000):\n",
    "    index = np.random.randint(0, len(X_set), length)\n",
    "    return X_set[index], y_set[index]\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = {}\n",
    "n_estimators = 25\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    x, y = random_sample(images_train, labels_train)\n",
    "    log_clf = LogisticRegression(solver = 'newton-cg', multi_class = 'auto', max_iter = 500)\n",
    "    log_clf.fit(x, y)\n",
    "    clfs['clf_' + str(i)] = log_clf\n",
    "    print('clf trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard and soft voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define three functions used to extract the hard and soft predictions of the ensemble to compare them with the 'tuned' one. For th soft voting predictions i am going to need also the predicted probabilities of every classifiers for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_hard_predictions(clfs, x_set):\n",
    "    ensemble_hard_pred = []\n",
    "    for instance in x_set:\n",
    "        instance_pred = []\n",
    "        for i in range(n_estimators):\n",
    "            prediction = int(clfs['clf_' + str(i)].predict(instance.reshape(1, -1)))\n",
    "            instance_pred.append(prediction)\n",
    "        ensemble_hard_pred.append(max(set(instance_pred), key = instance_pred.count))\n",
    "    ensemble_hard_pred = np.array(ensemble_hard_pred)\n",
    "    return ensemble_hard_pred\n",
    "\n",
    "def return_class(predictions):\n",
    "    return np.where(predictions[0] == max(predictions[0]))\n",
    "\n",
    "def return_soft_predictions(clfs, x_set):\n",
    "    ensemble_soft_pred = []\n",
    "    for instance in images_valid:\n",
    "        instance_pred_proba = []\n",
    "        for i in range(n_estimators):\n",
    "            prediction = clfs['clf_' + str(i)].predict_proba(instance.reshape(1, -1))\n",
    "            instance_pred_proba.append(prediction)\n",
    "        instance_pred = np.mean(np.array(instance_pred_proba), axis = 0)\n",
    "        ensemble_soft_pred.append(return_class(instance_pred))\n",
    "    ensemble_soft_pred = np.array(ensemble_soft_pred).reshape(labels_valid.shape)\n",
    "    return ensemble_soft_pred\n",
    "\n",
    "ensemble_hard_pred = return_hard_predictions(clfs, images_valid)\n",
    "ensemble_soft_pred = return_soft_predictions(clfs, images_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use accuracy as evaluation metrics. I am going to compare it also with the null accuracy, in order to see in my ensembles are above the minimum threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard voting ensemble accuracy:  0.84\n",
      "Soft voting ensemble accuracy:  0.8407\n",
      "Null accuracy:  0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_hard_voting = accuracy_score(ensemble_hard_pred, labels_valid)\n",
    "acc_soft_voting = accuracy_score(ensemble_soft_pred, labels_valid)\n",
    "print('Hard voting ensemble accuracy: ', acc_hard_voting)\n",
    "print('Soft voting ensemble accuracy: ', acc_soft_voting)\n",
    "\n",
    "lst_labels = list(labels_train)\n",
    "mode_train = max(set(lst_labels), key = lst_labels.count)\n",
    "y_null = np.zeros(labels_valid.shape) + mode_train\n",
    "null_accuracy = accuracy_score(y_null, labels_valid)\n",
    "print('Null accuracy: ', null_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the introduction, the soft ensemble led to better perfomances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the weights of the soft probabilities I am gonna need an additional set. The idea is to split again the original training set into a train set for the estimators and a train set for the neural network.\n",
    "Since I am going to modify the training space for my estimators I will compute again the accuracy score for both of the ensemble methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "images_train_, images_nn, labels_train_, labels_nn = train_test_split(images_train, labels_train,\n",
    "                                                                      test_size = 0.4, random_state = 42)\n",
    "images_train_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to train the estimators again as well as the accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n",
      "clf trained\n"
     ]
    }
   ],
   "source": [
    "clfs_ = {}\n",
    "n_estimators_ = 25\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    x, y = random_sample(images_train_, labels_train_)\n",
    "    log_clf = LogisticRegression(solver = 'newton-cg', multi_class = 'auto', max_iter = 500)\n",
    "    log_clf.fit(x, y)\n",
    "    clfs_['clf_' + str(i)] = log_clf\n",
    "    print('clf trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard voting ensemble accuracy:  0.8398\n",
      "Soft voting ensemble accuracy:  0.8412\n"
     ]
    }
   ],
   "source": [
    "ensemble_hard_pred_nn = return_hard_predictions(clfs_, images_valid)\n",
    "ensemble_soft_pred_nn = return_soft_predictions(clfs_, images_valid)\n",
    "acc_hard_voting = accuracy_score(ensemble_hard_pred_nn, labels_valid)\n",
    "acc_soft_voting = accuracy_score(ensemble_soft_pred_nn, labels_valid)\n",
    "print('Hard voting ensemble accuracy: ', acc_hard_voting)\n",
    "print('Soft voting ensemble accuracy: ', acc_soft_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction of the training set did not affect significantly the perfomance of the ensemble. On the opposite the soft voting led to a general improve of the accuracy, but we can impute this to the randomness of the training process (the single sample on which a classifier is trained is draw randomly). Now I need to create the inputs set for the neural network. Note that the following function can be also used for returning the classic soft voting predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_soft_predictions(clfs, x_set, nn_inputs = False):\n",
    "    ensemble_soft_pred = []\n",
    "    ensemble_soft_proba = []\n",
    "    for instance in x_set:\n",
    "        instance_pred_proba = []\n",
    "        for i in range(n_estimators):\n",
    "            prediction = clfs['clf_' + str(i)].predict_proba(instance.reshape(1, -1))\n",
    "            instance_pred_proba.append(prediction)\n",
    "        if nn_inputs:\n",
    "            ensemble_soft_proba.append(instance_pred_proba)\n",
    "            ensemble_soft_pred.append(return_class(instance_pred_proba))\n",
    "        else:\n",
    "            instance_pred = np.mean(np.array(instance_pred_proba), axis = 0)\n",
    "            ensemble_soft_proba.append(instance_pred)\n",
    "            ensemble_soft_pred.append(return_class(instance_pred))\n",
    "              \n",
    "    ensemble_soft_proba = np.array(ensemble_soft_proba).reshape(x_set.shape[0], n_estimators * 10)\n",
    "    return ensemble_soft_pred, ensemble_soft_proba\n",
    "\n",
    "labels_nn_one_hot = np.zeros((labels_nn.shape[0], 10))\n",
    "for i in range(labels_nn.shape[0]):\n",
    "    labels_nn_one_hot[i, labels_nn[i]] = 1\n",
    "    \n",
    "labels_valid_one_hot = np.zeros((labels_valid.shape[0], 10))\n",
    "for i in range(labels_valid_one_hot.shape[0]):\n",
    "    labels_valid_one_hot[i, labels_valid[i]] = 1\n",
    "    \n",
    "nn, nn_inputs = return_soft_predictions(clfs, images_nn, True)\n",
    "nn, nn_valid = return_soft_predictions(clfs, images_valid, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to build, compile and train the neural network. In this section there are a lot of decision variable to take into account to minimize the final loss (number of hidden layers, number of neuron, activation functions, optimizer...). The parameters i decided to use are not the output of any sort of tuning: this means that potentially there is still space for accuracy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 2.2535 - accuracy: 0.8262 - val_loss: 1.7266 - val_accuracy: 0.4217\n",
      "Epoch 2/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.6946 - accuracy: 0.8490 - val_loss: 0.8024 - val_accuracy: 0.8260\n",
      "Epoch 3/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.5802 - accuracy: 0.8535 - val_loss: 0.6838 - val_accuracy: 0.8344\n",
      "Epoch 4/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.5365 - accuracy: 0.8568 - val_loss: 0.7785 - val_accuracy: 0.8267\n",
      "Epoch 5/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.5101 - accuracy: 0.8575 - val_loss: 0.5657 - val_accuracy: 0.8363\n",
      "Epoch 6/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4963 - accuracy: 0.8580 - val_loss: 0.6011 - val_accuracy: 0.8353\n",
      "Epoch 7/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4841 - accuracy: 0.8586 - val_loss: 0.5869 - val_accuracy: 0.8381\n",
      "Epoch 8/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4690 - accuracy: 0.8605 - val_loss: 0.5927 - val_accuracy: 0.8276\n",
      "Epoch 9/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4542 - accuracy: 0.8627 - val_loss: 0.5344 - val_accuracy: 0.8374\n",
      "Epoch 10/60\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4564 - accuracy: 0.8613 - val_loss: 0.5551 - val_accuracy: 0.8424\n",
      "Epoch 11/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4483 - accuracy: 0.8612 - val_loss: 0.4966 - val_accuracy: 0.8428\n",
      "Epoch 12/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4416 - accuracy: 0.8630 - val_loss: 0.4908 - val_accuracy: 0.8434\n",
      "Epoch 13/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4336 - accuracy: 0.8651 - val_loss: 0.5170 - val_accuracy: 0.8374\n",
      "Epoch 14/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4295 - accuracy: 0.8633 - val_loss: 0.4990 - val_accuracy: 0.8370\n",
      "Epoch 15/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4271 - accuracy: 0.8648 - val_loss: 0.5283 - val_accuracy: 0.8390\n",
      "Epoch 16/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4223 - accuracy: 0.8644 - val_loss: 0.4978 - val_accuracy: 0.8417\n",
      "Epoch 17/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4165 - accuracy: 0.8655 - val_loss: 0.4752 - val_accuracy: 0.8442\n",
      "Epoch 18/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4160 - accuracy: 0.8659 - val_loss: 0.4886 - val_accuracy: 0.8414\n",
      "Epoch 19/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4117 - accuracy: 0.8665 - val_loss: 0.4909 - val_accuracy: 0.8401\n",
      "Epoch 20/60\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4082 - accuracy: 0.8658 - val_loss: 0.4709 - val_accuracy: 0.8444\n",
      "Epoch 21/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4075 - accuracy: 0.8675 - val_loss: 0.4665 - val_accuracy: 0.8439\n",
      "Epoch 22/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4068 - accuracy: 0.8671 - val_loss: 0.4638 - val_accuracy: 0.8452\n",
      "Epoch 23/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4020 - accuracy: 0.8668 - val_loss: 0.4788 - val_accuracy: 0.8433\n",
      "Epoch 24/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3983 - accuracy: 0.8670 - val_loss: 0.4857 - val_accuracy: 0.8431\n",
      "Epoch 25/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3972 - accuracy: 0.8681 - val_loss: 0.4838 - val_accuracy: 0.8430\n",
      "Epoch 26/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3999 - accuracy: 0.8669 - val_loss: 0.4674 - val_accuracy: 0.8413\n",
      "Epoch 27/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3992 - accuracy: 0.8659 - val_loss: 0.4674 - val_accuracy: 0.8459\n",
      "Epoch 28/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3978 - accuracy: 0.8673 - val_loss: 0.4889 - val_accuracy: 0.8403\n",
      "Epoch 29/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3961 - accuracy: 0.8678 - val_loss: 0.4745 - val_accuracy: 0.8430\n",
      "Epoch 30/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3933 - accuracy: 0.8668 - val_loss: 0.4609 - val_accuracy: 0.8473\n",
      "Epoch 31/60\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3944 - accuracy: 0.8700 - val_loss: 0.4711 - val_accuracy: 0.8429\n",
      "Epoch 32/60\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3902 - accuracy: 0.8695 - val_loss: 0.4697 - val_accuracy: 0.8442\n",
      "Epoch 33/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3896 - accuracy: 0.8684 - val_loss: 0.4686 - val_accuracy: 0.8439\n",
      "Epoch 34/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3869 - accuracy: 0.8696 - val_loss: 0.4573 - val_accuracy: 0.8448\n",
      "Epoch 35/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3896 - accuracy: 0.8709 - val_loss: 0.4554 - val_accuracy: 0.8447\n",
      "Epoch 36/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3880 - accuracy: 0.8695 - val_loss: 0.4641 - val_accuracy: 0.8463\n",
      "Epoch 37/60\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3874 - accuracy: 0.8690 - val_loss: 0.4618 - val_accuracy: 0.8447\n",
      "Epoch 38/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3857 - accuracy: 0.8714 - val_loss: 0.4839 - val_accuracy: 0.8415\n",
      "Epoch 39/60\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3835 - accuracy: 0.8714 - val_loss: 0.4640 - val_accuracy: 0.8438\n",
      "Epoch 40/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3827 - accuracy: 0.8695 - val_loss: 0.4619 - val_accuracy: 0.8470\n",
      "Epoch 41/60\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3851 - accuracy: 0.8693 - val_loss: 0.4719 - val_accuracy: 0.8424\n",
      "Epoch 42/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3832 - accuracy: 0.8698 - val_loss: 0.4590 - val_accuracy: 0.8443\n",
      "Epoch 43/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3806 - accuracy: 0.8711 - val_loss: 0.4729 - val_accuracy: 0.8440\n",
      "Epoch 44/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3833 - accuracy: 0.8706 - val_loss: 0.4603 - val_accuracy: 0.8452\n",
      "Epoch 45/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3802 - accuracy: 0.8703 - val_loss: 0.4848 - val_accuracy: 0.8417\n",
      "Epoch 46/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3793 - accuracy: 0.8716 - val_loss: 0.4608 - val_accuracy: 0.8448\n",
      "Epoch 47/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3794 - accuracy: 0.8715 - val_loss: 0.4749 - val_accuracy: 0.8430\n",
      "Epoch 48/60\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3780 - accuracy: 0.8718 - val_loss: 0.4615 - val_accuracy: 0.8449\n",
      "Epoch 49/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3784 - accuracy: 0.8713 - val_loss: 0.4559 - val_accuracy: 0.8461\n",
      "Epoch 50/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3768 - accuracy: 0.8713 - val_loss: 0.4628 - val_accuracy: 0.8463\n",
      "Epoch 51/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3769 - accuracy: 0.8718 - val_loss: 0.4586 - val_accuracy: 0.8437\n",
      "Epoch 52/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3774 - accuracy: 0.8701 - val_loss: 0.4715 - val_accuracy: 0.8432\n",
      "Epoch 53/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3777 - accuracy: 0.8717 - val_loss: 0.4590 - val_accuracy: 0.8447\n",
      "Epoch 54/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3753 - accuracy: 0.8699 - val_loss: 0.4648 - val_accuracy: 0.8450\n",
      "Epoch 55/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3749 - accuracy: 0.8713 - val_loss: 0.4704 - val_accuracy: 0.8456\n",
      "Epoch 56/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3732 - accuracy: 0.8726 - val_loss: 0.4780 - val_accuracy: 0.8445\n",
      "Epoch 57/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3739 - accuracy: 0.8723 - val_loss: 0.4758 - val_accuracy: 0.8456\n",
      "Epoch 58/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3729 - accuracy: 0.8719 - val_loss: 0.4619 - val_accuracy: 0.8432\n",
      "Epoch 59/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3715 - accuracy: 0.8723 - val_loss: 0.4650 - val_accuracy: 0.8445\n",
      "Epoch 60/60\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3750 - accuracy: 0.8715 - val_loss: 0.4720 - val_accuracy: 0.8442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14a439a5f98>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([keras.layers.Dense(200, input_shape = [25 * 10]),\n",
    "                                 keras.layers.Dense(150, activation = 'elu', kernel_initializer = 'he_normal',\n",
    "                                                    kernel_regularizer = keras.regularizers.l2(0.02)),\n",
    "                                 keras.layers.BatchNormalization(),\n",
    "                                 keras.layers.Dense(150, activation = 'elu', kernel_initializer = 'he_normal',\n",
    "                                                    kernel_regularizer = keras.regularizers.l2(0.02)),\n",
    "                                 keras.layers.BatchNormalization(),\n",
    "                                 keras.layers.Dense(150, activation = 'elu', kernel_initializer = 'he_normal',\n",
    "                                                    kernel_regularizer = keras.regularizers.l2(0.02)),\n",
    "                                 keras.layers.BatchNormalization(),\n",
    "                                 keras.layers.Dense(10, activation = 'softmax')])\n",
    "optimizer = keras.optimizers.Adam(lr = 5e-2, beta_1 = 0.9, beta_2 = 0.999, decay = 1e-2)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "model.fit(n, labels_nn_one_hot, batch_size = 64, epochs = 60,\n",
    "          validation_data = (nn_valid, labels_valid_one_hot))\n",
    "nn_tuning_acc = model.evaluate(nn_valid, labels_valid_one_hot)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard voting ensemble accuracy:  0.8398\n",
      "Soft voting ensemble accuracy:  0.8412\n",
      "Neural ensemble accuracy:  0.8442\n"
     ]
    }
   ],
   "source": [
    "print('Hard voting ensemble accuracy: ', acc_hard_voting)\n",
    "print('Soft voting ensemble accuracy: ', acc_soft_voting)\n",
    "print('Neural ensemble accuracy: ', round(nn_tuning_acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and future improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural ensemble achieved significantly higher perfomance rather than traditional voting methods. Even if at a first look the improvement does not seem so remarkable, We have to keep in mind that the different ensemble criteria are build at the top of the same classifiers: this means that It will be impossible to boost global perfomance beyond a certain threshold. The downsize of the neural ensemble is that in order to implement it we need a consistent number of observations in the training set, since it will be split in two sub-set (my initial training set had 60000 images). The general suggestion to apply the neural ensemble of smaller training set is to prioritize the training of the classifiers, because they are the main driver of overall accuracy, and of course applying regularization measures to avoid overfitting.\n",
    "\n",
    "The next step will be to wrap everything together to build a class, in order to be able to use the neural ensemble as a traditional SciKit-learn estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
